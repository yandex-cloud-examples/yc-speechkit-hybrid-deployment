# Развертывание решения SpeechKit Hybrid в Yandex Cloud с помощью Terraform 

## Оглавление
* [Введение](#about)
* [Среда исполнения](#runtime)
* [Развертывание решения SK-Hybrid](#deploy)
  * [Подготовка инфраструктуры в Yandex Cloud для развертывания решения](#yc-prep)
  * [Развертывание решения `SK-Hybrid` с помощью Terraform](#deploy-tf)
  * [Проверка работоспособности развёрнутого решения](#test)
* [Поиск несправностей](#diagnostic)

## Введение <a id="about"/></a>

В данном решении представлено развертывание технологии [SpeechKit Hybrid](https://cloud.yandex.ru/docs/speechkit/hybrid-speechkit/) (далее `SK-Hybrid`) для ознакомительных целей в инфраструктуре `Yandex Cloud` с помощью инструмента [Terraform](https://www.terraform.io/). 

Технологию `SK-Hybrid` можно развернуть в любой современной инфраструктуре - как на обычном физическом сервере, так и в виртуальной машине на облачной платформе (`Azure`, `AWS`, `GCP`, `Selectel`, `Yandex Cloud` и т.д.) под управлением операционной системы Linux на базе дистрибутива от [Debian](https://www.debian.org) или [RHEL](https://www.redhat.com/). Необходимым условием для запуска решения является соответствие среды развертывания [системным требованиям](https://cloud.yandex.ru/docs/speechkit-hybrid/system-requirements), описанным в документации.


## Среда исполнения <a id="runtime"/></a>

Решение `SK-Hybrid` должно запускаться в [среде контейнеризации Docker Engine](https://cloud.yandex.ru/docs/speechkit-hybrid/system-requirements). Сегодня в мире есть несколько реализаций `Docker Engine` и наиболее известные из них следующие:
- [Docker](https://www.docker.com/).
- [Podman](https://podman.io/).
- [Kubernetes](https://kubernetes.io/). 
- [OpenShift](https://www.redhat.com/en/technologies/cloud-computing/openshift).

Для запуска контейнеров `SK-Hybrid` требуются [определенные графические карты c GPU](https://cloud.yandex.ru/docs/speechkit-hybrid/system-requirements#hardware)

Производитель графических карт, компания [NVIDIA](www.nvidia.com) требует для работы своих драйверов и системного ПО [определенных версий ОС](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/supported-platforms.html). Технически возможно настроить драйвера и системное ПО для GPU для работы в операционных системах, не входящих в список поддерживаемых производителем, но в этом случае это будет неподдерживаемая произодителем схема, в которой работоспособность всех компонентов не гарантируется.


## Развертывание решения <a id="deploy"/></a>

### Подготовка инфраструктуры в Yandex Cloud <a id="yc-prep"/></a>

В процессе подготовки к развёртыванию `SK-Hybrid` понадобится сделать следующее:

  1. Наладить первичное подключение к "Yandex Cloud" и установить необходимые утилиты.
  2. Создать проект в "Yandex Cloud" и выделенного сервисного пользователя для этого проекта.
  3. Подготовить ключ аутентификации для аутентификации в биллинге "Yandex Cloud".
  4. Подготовить SSH-ключи для подключения к развёртываемым несущим нодам.

#### Первичное подключение к "Yandex Cloud" и установка утилит

Чтобы пользоваться инфраструктурой "Yandex Cloud" потребуется пройти следующие этапы регистрации, установки и настройки необходимого ПО:

- [Регистрация в "Yandex-ID"](https://yandex.ru/support/id/authorization/registration.html)
- [Вход в web-консоль "Yandex Cloud"](https://cloud.yandex.ru/docs/overview/quickstart)
- [Установка утилиты `yc`](https://cloud.yandex.ru/docs/cli/operations/install-cli)
- [Аутентификация с помощью `yc` в "Yandex Cloud"](https://cloud.yandex.ru/docs/cli/operations/authentication/user)
- [Установка утилиты "terraform"](https://cloud.yandex.ru/docs/tutorials/infrastructure-management/terraform-quickstart)

#### Создание проекта в "Yandex Cloud" и выделенного сервисного пользователя

Принимаем как данность, что все пути локального проекта указываются относительно корня terraform-рецепта "**~/sk-hybrid-example**".

Для экспериментов в каждом отдельном проекте ("каталоге", "папке", "folder", в терминологии "Yandex Cloud") очень желательно создавать выделенного "сервисного" пользователя (сущность "Yandex Cloud"), права доступа которого не пересекаются с другими проектами, что отчасти ограждает нас от деструктивных ошибок.

С функциональной точки зрения наименование проекта и сервисного аккаунта (пользователя) неважно, но для определённости в нашем пробном стенде назовём их одинаково - "**sk-hybrid-example**".

Подготавливаем структуру для размещения проекта и выделенного пользователя с полными (или достаточным) правами доступа к этой структуре:

- [Создание проекта в "Yandex Cloud"](https://cloud.yandex.ru/docs/resource-manager/operations/folder/create)
- [Создание "сервисного аккаунта" в "Yandex Cloud"](https://cloud.yandex.ru/docs/iam/operations/sa/create)
- [Выделение "сервисному аккаунту" роли "admin" проекту "sk-hybrid-example"](https://cloud.yandex.ru/docs/iam/operations/sa/assign-role-for-sa)
- [Аутентификация "yc" в "Yandex Cloud" от имени сервисного аккаунта](https://cloud.yandex.ru/docs/cli/operations/authentication/service-account#auth-as-sa)

После создания проекта в "Yandex Cloud" и сервисного аккаунта, и выделения последнему доступа к проекту следует проверить, действительно ли утилита "yc", переключившись в профиль выделенного сервисного аккаунта имеет требуемый уровень доступа к проекту.

Убеждаемся, что локальный профиль "yc" создан:

```bash
$ yc config profile list
```

Активируем профиль сервисного аккаунта:

```bash
$ yc config profile activate sk-hybrid-example
```

Выясняем идентификатор сервисного аккаунта:

```bash
$ yc iam service-account get sk-hybrid-example --folder-name sk-hybrid-example
```
Запомним вывод - он нам понадобится далее:

```text
id: as9...abs
....
name: sk-hybrid-example
```

Запрашиваем перечень разрешений доступа к проекту пробного стенда:

```bash
$ yc resource-manager folder list-access-bindings sk-hybrid-example
```

Получаем как минимум такой перечень доступных текущему пользователю ролей:

```text
+---------+----------------+------------+
| ROLE ID |  SUBJECT TYPE  | SUBJECT ID |
+---------+----------------+------------+
| admin   | serviceAccount | as9...abs  |
+---------+----------------+------------+
```

#### Об используемых операционных системах (ОС)

В репозитории "Yandex Cloud" есть публично доступные образы ОС, уже подготовленные для запуска в них контейнеров "Docker", а также с установленными драйверами и утилитами GPU "Nvidia" - для нашего простейшего пробного стенда воспользуемся ими.

#### Подготовка ключа аутентификации для "YC Billing"

Для контроля легитимности инсталляции в SK-Hybrid имеется два механизма: предоставление лимитированной лицензии на аппаратном usb-носителе "Yubikey" и учёт фактического потребления посредством привязки инсталляции к учётной записи в "Yandex Cloud", с отправкой в централизованный биллинг метрик о каждой транзакции, по которым впоследствии выставляются счета на оплату.

В этом пробном стенде мы рассматриваем схему с "облачным биллингом" - второй вариант, из упомянутых выше двух.

Аутентификация SK-Hybrid в "Yandex Cloud Billing" осуществляется посредством строкового статического API-ключа. Создать api-ключ можно [разными способами](https://cloud.yandex.ru/docs/iam/operations/api-key/create) - например,так:

```bash
$ yc iam api-key create --service-account-name sk-hybrid-example --description "Example SpeechKit-Hybrid Cloud-Billing Gateway API"
```

Результатом будет идентификатор и секретная часть ключа. Последнюю впоследствии нужно будет подавать SK-Hybrid в переменной "BILLING_STATIC_API_KEY".

Важно иметь в виду, что для работы SK-Hybrid с лицензированием через "облачный биллинг" потребуется обеспечить от сервера (контейнера) с "billing-agent" и "license-server" сетевую связность с узлом "billing.datasphere.yandexcloud.net:443".

Проверить доступность сервиса "облачного биллинга" можно командой "nc -vz billing.datasphere.yandexcloud.net 443" (ожидаемый ответ "Connection to billing.datasphere.yandexcloud.net 443 port [tcp/https] succeeded!").

#### Подготовка SSH-ключей для подключения к несущим нодам

Как минимум, terraform-рецепту понадобится указать на публичную часть ssh-ключа, который будет загружен на несущие ноды SK-Hybrid, чтобы впоследствии туда можно было ходить для отладки и просмотра журналов событий:

```bash
$ ln -s ~/.ssh/id_rsa.pub ./keys/ssh-user-id-rsa.pub
```

### Сборка схемы и развёртывание <a id="deploy-tf"/></a>

Попытка рассказывать что-либо о логике работы terraform-рецепта будет очень слабой заменой чтению самого рецепта, подробно и в мелких шагах описывающего, как и в каких зависимостях создаются и конфигурируются необходимые сущности.

Отмечу, что развёртывание SK-Hybrid как такового производится инструкциями в файле "node-deploy.tf", а всё остальное - подготовка инфраструктуры в "Yandex Cloud". Это хорошо показывает, насколько просто запускается сам сервис SK-Hybrid.

Далее для развёртывания SK-Hybrid понадобится сделать как минимум следующее:

  1. Создать конфигурационный файл и задать параметры terraform-рецепта.
  2. Задать переменные окружения конфигурации провайдера "terrafom".
  3. Активировать профиль сервисного пользователя проекта и анонсировать переменные окружения.
  4. Проверить синтаксическую корректность terraform-рецепта и запустить его

#### Конфигурирование переменных terraform-рецепта

В этом terraform-рецепте зависимые от клиента и секретные данные вынесены в переменные окружения, подаваемые утилитам "terraform" и "yc" только во время их использования. Перед запуском необходимо заполнить конфигурационный файл с переменными, создав его из прилагаемого шаблона:

```bash
$ cp ./terraform.tfvars.template ./terraform.tfvars
$ vi ./terraform.tfvars
```

Назначение обязательных переменных следующее:

| Name | Description |
| ---- | ----------- |
| CR_ENDPOINT | адрес выделенного клиентского "Container Registry" (например "cr.yandex") |
| CR_REGISTRY_ID | идентификатор выделенного клиентского "Container Registry" (без FQDN репозитория, вроде "cr.yandex") |
| BILLING_STATIC_API_KEY | api-ключ для аутентификации в "YC Billing", связывающий инсталляцию SK-Hybrid с учётной записью в "Yandex Cloud" |
| SKH_VER | версия релиза "SpeechKit-Hybrid" |

Из соображений экономии ресурсов в этом terraform-рецепте дорогостоящие инстансы с GPU конфигурируются в режиме разрешения их автоматической остановки по истечении 24 часов с момента запуска или при исчерпании свободных ресурсов в дата центре - это поведение можно отключить изменением соответствующего флага в "terraform.tfvars" (прерываемые виртуальные машины стоят примерно в четыре раза дешевле):

| Name | Description |
| ---- | ----------- |
| NODES_GPU_INTERRUPTIBLE | разрешено автоматически останавливать инстансы с GPU (default: true) |

#### Конфигурирование переменных окружения для провайдера "terrafom"

Провайдер "terraform" для работы с "Yandex Cloud" принимает конфигурационные переменные из переменных окружения, выбирая из них те, что начинаются префиксом "YC_".

Назначение переменных конфигурации подключения terraform-провайдера к "Yandex Cloud":

| Name | Description |
| ---- | ----------- |
| YC_TOKEN | короткоживущий (12 часов) IAM-токен для аутентификации в "Yandex Cloud" |
| YC_CLOUD_ID | идентификатор клиентского облака ("cloud-id") в "Yandex Cloud" |
| YC_FOLDER_ID | идентификатор проекта ("folder-id") в "Yandex Cloud" |

Учитывая то, что ключи аутентификации в "Yandex Cloud" привязаны к профилю ранее заведённого для проекта сервисного пользователя, явно переключаемся в контекст такового:

```bash
$ yc config profile list
$ yc config profile activate sk-hybrid-example
```

Анонсируем переменные конфигурации подключения terraform-провайдера к "Yandex Cloud":

```bash
$ { export YC_TOKEN=$(yc iam create-token);
    export YC_CLOUD_ID=$(yc config get cloud-id);
    export YC_FOLDER_ID=$(yc config get folder-id); }
```

Желательно время от времени надо проверять, указаны ли в профиле сервисного пользователя идентификаторы тех самых "облака" и "проекта", в которые мы целимся - во избежание:

```bash
$ yc config list
```

#### Запуск terraform-рецепта

Перед развёртыванием инфраструктуры посредством "terraform" локальный проект требуется инициализировать (в процессе этого будут загружены необходимые модули и произведена поверхностная проверка корректности структуры terraform-рецепта):

```bash
$ terraform init
```

Перед применением terraform-рецепта важно проверять, что будет изменено в уже существующей инфраструктуре при его исполнении:

```bash
$ terraform plan
```

После того, как предлагаемый план будет вычитан и не вызовет возражений, применяем изменения:

```bash
$ terraform apply
```

### Проверка работоспособности развёрнутого решения <a id="test"/></a>

#### Ознакомление с перечнем созданных ресурсов

Активируем профиль сервисного аккаунта, переключаясь в контекст, в котором развёрнут пробный стенд:

```bash
$ yc config profile activate sk-hybrid-example
```

Получаем список виртуальных машин в проекте:

```bash
$ yc compute instance list
```

К ip-адресу виртуальной машины, сведения о которой мы выше получили, мы будем подключаться в дальнейшем, в процессе тестирования функционала пробного стенда.

#### Тестирования функционала распознавания и синтеза

Проверяем, открыты ли порты для приёма клиентских запросов:

```bash
$ telnet <sk-hybrid-ip> 8080
$ telnet <sk-hybrid-ip> 9080
```

Запускаем синтетический тест распознавания, используя для этого специализированную утилиту от разработчиков SK-Hybrid:

```bash
$ docker run --rm --name stt-tools --env ENVOY_HOST=<sk-hybrid-ip> --env ENVOY_PORT=8080 --env CONNECTIONS=40 --env ERRORS_THRESHOLD=0.1 --env SEND_MODE=RealTime <cr-endpoint>/<cr-id>/release/tools/stt-tools:0.20
```

Запускаем синтетический тест синтеза:

```bash
$ docker run --rm --network=host --name tts-tools --env ENVOY_HOST=<sk-hybrid-ip> --env ENVOY_TTS_PORT=9080 --env RPS=20 <cr-endpoint>/<cr-id>/release/tools/tts-tools:0.20
```

Контейнеры с "stt/tts-tools" весьма простые и утилитарные - они не реагируют на сигналы прерывания процесса "^C", а рабочий цикл довольно длительный, так что иногда контейнеры приходится явно останавливать, чтобы зря не расходовать ресурсы:

```bash
$ docker stop stt-tools
$ docker stop tts-tools
```

## Поиск несправностей <a id="diagnostic"/></a>

Если что-то пошло не так.

### Подключение к несущим инстансам посредством SSH

Пройти на виртуальную машину с SK-Hybrid можно так:

```bash
$ ssh terraform@<node-ip>
```

Если для подключения к инстансу был выбран какой-то специфический ssh-ключ, следует на него указать, примерно так:

```bash
$ ssh -i ./.ssh/custom-key/id_rsa terraform@<node-ip>
```

### Перечень используемых сетевых портов

В SK-Hybrid используется только TCP-протокол.

Порт для исходящих подключений к "Yandex Billing":

 * 443 - YC-Billing (вне схемы SK-Hybrid, но доступ от "License-Server" требуется)

Порты неразрывного комплекта микросервисов "Envoy + License-Server":

 * 8080 - Envoy STT gRPC entry point (from clients)
 * 9080 - Envoy TTS gRPC entry point (from clients)
 * 9091 - Envoy gRPC API (controlled by License-Server)
 * 8087 - License-Server point for STT instances
 * 9087 - License-Server point for TTS instances

Порты миросервиса STT:

 * 17880 - STT gRPC payload point (default: 17001)
 * 17882 - Metrics of STT (default: 17002)

Порты микросервиса TTS:

 * 17980 - TTS gRPC payload point (default: 17001)
 * 17982 - Metrics of TTS (default: 17002)

В этом примере порты микросервисов STT/TTS при запуске разносятся (встроенными в SK-Hybrid средствами) в разные значения для возможности работы на одном сетевом интерфейсе.

### Признаки успешности запуска компонентов SK-Hybrid

Важно учитывать, что на загрузку "нейронных карт" распознавания/синтеза требуется некоторое время, от двух до десяти минут, в зависимости от мощности несущего сервера. Индикатор успешности загрузки - строка вроде "Load finished. Ready to serve requests on 0.0.0.0:17001" в журнале STT/TTS-сервера. До этого момента сервисы распознавания/синтеза не будут отвечать на запросы.

Критерием успешности запуска и готовности SK-Hybrid обслуживать клиентские запросы служит начало прослушивания портов микросервисом "Envoy" (по умолчанию это TCP:8080 (STT) и TCP:9080 (TTS)).


### Примерная траектория обзора в поисках места сбоя

Проверяем, открыты ли порты приёма клиентских запросов:

```bash
$ telnet <sk-hybrid-ip> 8080
$ telnet <sk-hybrid-ip> 9080
```

Смотрим на перечень загруженных образов и запущенных контейнеров:

```bash
$ docker images --digests
$ docker ps -a
```

Смотрим на перечень активных процессов, имеющих отношение в контейнеризации:

```bash
$ sudo ps waxu | grep -i 'docker'
```

Смотрим на перечень открытых сетевых соединений и сетевую конфигурацию:

```bash
$ sudo netstat -tulpn
$ sudo ip addr
```

Смотрим на состояние драйвера GPU и интеграции такового с системой контейнеризации:

```bash
$ cat /proc/driver/nvidia/version
$ sudo nvidia-smi
$ nvidia-container-cli info
```

Выгружаем и просматриваем вывод STDOUT контейнеров:

```bash
$ mkdir -p logs ; cd ./logs
$ for c in $(docker ps --format '{{.Names}}' | awk '{print $NF}'); do echo $c && docker logs $c &> $c.log; done
```

Выгружаем и просматриваем текущую конфигурацию "Envoy":

```bash
$ curl http://localhost:9091/config_dump > ./logs/envoy_config_dump.json
```

Изучаем содержимое файла "docker-compose.yaml", посредством которого осуществляется запуск.
